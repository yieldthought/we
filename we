#!/usr/bin/env python3
from __future__ import annotations

import argparse
import hashlib
import json
import math
import os
import re
import struct
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Set, Tuple
from urllib.parse import urlparse

from huggingface_hub import HfApi, HfFileSystem, get_token, hf_hub_download
from huggingface_hub.utils import HfHubHTTPError, disable_progress_bars


@dataclass
class TensorInfo:
    shape: Tuple[int, ...]
    dtype: str
    nbytes: int


@dataclass
class TrieNode:
    children: Dict[str, "TrieNode"] = field(default_factory=dict)
    tensor: Optional[TensorInfo] = None


@dataclass
class DisplayRow:
    label: str
    shape: Optional[str] = None
    dtype: Optional[str] = None
    size: Optional[str] = None
    percent: Optional[str] = None
    min_value: Optional[str] = None
    mean_value: Optional[str] = None
    max_value: Optional[str] = None
    compression_metrics: Dict[str, Tuple[str, str]] = field(default_factory=dict)


@dataclass
class IndexGroup:
    indices: List[int]
    representative: TrieNode


@dataclass
class MetadataCacheEntry:
    files: List[str]
    tensors: Dict[str, TensorInfo]
    tensor_files: Dict[str, str]
    remote_revision_id: str
    revision_checked_at_epoch: int


@dataclass
class TensorCompressionStats:
    min_val: float
    mean_val: float
    max_val: float
    pcc: float
    atol: float


@dataclass
class SourceState:
    source_label: str
    is_local: bool
    local_base_dir: Optional[Path]
    repo_id: Optional[str]
    revision: Optional[str]
    hf_token: Optional[str]


def supports_color() -> bool:
    if os.getenv("NO_COLOR") is not None:
        return False
    if os.getenv("FORCE_COLOR") is not None:
        return True
    term = os.getenv("TERM", "").lower()
    if term in {"", "dumb"}:
        return False
    return sys.stdout.isatty()


COLOR_ENABLED = supports_color()

COLOR_DARK_GREY = 242
COLOR_WHITE = 15
COLOR_LIGHT_GREY = 250
COLOR_SHAPE = 153
COLOR_SHAPE_DARK = 67
COLOR_DTYPE = 151
COLOR_SIZE = 223
COLOR_PERCENT = 217
COLOR_PCC = 183
COLOR_ATOL = 111
COLOR_ATOL_DARK = 68

CACHE_SCHEMA_VERSION = 2
REVISION_CHECK_TTL_SECONDS = 3600

COMPRESS_FORMAT_SPECS = {
    "bfp8": {"dtype_attr": "bfloat8_b", "transpose_before_compress": False},
    "bfp8.T": {"dtype_attr": "bfloat8_b", "transpose_before_compress": True},
    "bfp4": {"dtype_attr": "bfloat4_b", "transpose_before_compress": False},
    "bfp4.T": {"dtype_attr": "bfloat4_b", "transpose_before_compress": True},
}
SUPPORTED_COMPRESS_FORMATS = list(COMPRESS_FORMAT_SPECS.keys())
COMPRESS_FORMAT_ALIASES = {format_name.lower(): format_name for format_name in SUPPORTED_COMPRESS_FORMATS}
COMPRESS_ALL_TOKEN = "all"


def paint(text: str, color_code: int) -> str:
    if not COLOR_ENABLED:
        return text
    return f"\x1b[38;5;{color_code}m{text}\x1b[0m"


def paint_dark(text: str) -> str:
    return paint(text, COLOR_DARK_GREY)


def paint_label(text: str, has_tensor: bool) -> str:
    base_color = COLOR_LIGHT_GREY if has_tensor else COLOR_WHITE
    if not COLOR_ENABLED:
        return text
    return paint(text, base_color)


def paint_shape(text: str) -> str:
    if not COLOR_ENABLED:
        return text
    if "x" not in text:
        return paint(text, COLOR_SHAPE)
    parts = text.split("x")
    rendered = paint(parts[0], COLOR_SHAPE)
    for part in parts[1:]:
        rendered += paint("x", COLOR_SHAPE_DARK) + paint(part, COLOR_SHAPE)
    return rendered


def paint_dtype(text: str) -> str:
    return paint(text, COLOR_DTYPE)


def paint_size(text: str) -> str:
    if not COLOR_ENABLED:
        return text
    try:
        number, unit = text.rsplit(" ", 1)
    except ValueError:
        return paint(text, COLOR_SIZE)
    return paint(number, COLOR_SIZE) + paint(f" {unit}", COLOR_DARK_GREY)


def paint_percent(text: str) -> str:
    if not COLOR_ENABLED:
        return text
    stripped = text.strip()
    if not stripped.endswith("%"):
        return paint(text, COLOR_PERCENT)
    number = stripped[:-1].strip()
    return paint(number, COLOR_PERCENT) + paint(" %", COLOR_DARK_GREY)


def paint_scientific_e(text: str) -> str:
    if not COLOR_ENABLED:
        return text
    return "".join(paint("e", COLOR_DARK_GREY) if char == "e" else char for char in text)


def paint_metric_number(text: str, color_code: int) -> str:
    if not COLOR_ENABLED:
        return text
    leading = len(text) - len(text.lstrip(" "))
    if leading == len(text):
        return text
    return text[:leading] + paint(text[leading:], color_code)


def paint_scientific_metric(text: str, color_code: int, e_color_code: int = COLOR_DARK_GREY) -> str:
    if not COLOR_ENABLED:
        return text
    leading = len(text) - len(text.lstrip(" "))
    prefix = text[:leading]
    body = text[leading:]
    rendered = ""
    for char in body:
        if char == "e":
            rendered += paint("e", e_color_code)
        else:
            rendered += paint(char, color_code)
    return prefix + rendered


def paint_total_size(text: str) -> str:
    if not COLOR_ENABLED:
        return text
    try:
        number, unit = text.rsplit(" ", 1)
    except ValueError:
        return paint(text, COLOR_PERCENT)
    return paint(number, COLOR_PERCENT) + paint(f" {unit}", COLOR_DARK_GREY)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        prog="we",
        description="Weight evaluation helper for Hugging Face checkpoints.",
    )
    parser.add_argument(
        "repo",
        nargs="?",
        help="Hugging Face model repo/URL or local model path.",
    )
    parser.add_argument(
        "filter_query",
        nargs="*",
        help="Optional filter: substring match, or exact torch-style path if dotted (e.g. model.layers.1).",
    )
    parser.add_argument(
        "--list",
        dest="list_repo",
        metavar="HF_URL_OR_REPO",
        help="List tensors from a Hugging Face model repo (legacy; default behavior no longer needs this flag).",
    )
    parser.add_argument(
        "--revision",
        default="main",
        help="Hugging Face revision to inspect (default: main).",
    )
    parser.add_argument(
        "-c",
        "--compress",
        action="append",
        metavar="FORMAT",
        help=(
            "Optional host-side compression round-trip to run on matched tensors. "
            "Repeat for multiple formats (e.g. -c bfp8 -c bfp4.T). "
            "Use -c all to run all supported formats."
        ),
    )
    return parser.parse_args()


def resolve_compress_formats(raw_values: Optional[List[str]]) -> List[str]:
    if not raw_values:
        return []

    seen: Set[str] = set()
    resolved: List[str] = []

    def add_once(format_name: str) -> None:
        if format_name not in seen:
            seen.add(format_name)
            resolved.append(format_name)

    for raw_value in raw_values:
        value = raw_value.strip()
        lowered = value.lower()
        if lowered == COMPRESS_ALL_TOKEN:
            for supported in SUPPORTED_COMPRESS_FORMATS:
                add_once(supported)
            continue

        canonical = COMPRESS_FORMAT_ALIASES.get(lowered)
        if canonical is None:
            supported = ", ".join(SUPPORTED_COMPRESS_FORMATS)
            raise ValueError(f"Unsupported compression format '{raw_value}'. Supported formats: {supported}")
        add_once(canonical)

    return resolved


def resolve_hf_token() -> Optional[str]:
    env_token = os.getenv("HF_TOKEN") or os.getenv("HUGGING_FACE_HUB_TOKEN")
    if env_token:
        stripped = env_token.strip()
        if stripped:
            return stripped
    cached_token = get_token()
    if isinstance(cached_token, str):
        stripped = cached_token.strip()
        if stripped:
            return stripped
    return None


def normalize_repo_id(raw_value: str) -> str:
    value = raw_value.strip()
    if not value:
        raise ValueError("Empty repo value.")

    if "://" not in value:
        return value.strip("/")

    parsed = urlparse(value)
    host = parsed.netloc.lower()
    if host.startswith("www."):
        host = host[4:]
    if host not in {"huggingface.co", "hf.co"}:
        raise ValueError(f"Unsupported host: {parsed.netloc}")

    parts = [part for part in parsed.path.split("/") if part]
    if not parts:
        raise ValueError("URL path does not contain a repo id.")

    if parts[0] in {"models", "model"}:
        parts = parts[1:]
    elif parts[0] in {"datasets", "spaces"}:
        raise ValueError("Only model repos are supported.")

    stop_tokens = {"tree", "blob", "resolve", "commit", "discussions"}
    for idx, part in enumerate(parts):
        if part in stop_tokens:
            parts = parts[:idx]
            break

    if len(parts) >= 2:
        return f"{parts[0]}/{parts[1]}"
    return parts[0]


def metadata_cache_dir() -> Path:
    env_dir = os.getenv("WE_CACHE_DIR")
    if env_dir:
        return Path(env_dir).expanduser() / "metadata"
    xdg_cache_home = os.getenv("XDG_CACHE_HOME")
    if xdg_cache_home:
        return Path(xdg_cache_home).expanduser() / "we" / "metadata"
    return Path.home() / ".cache" / "we" / "metadata"


def metadata_cache_path(repo_id: str, revision: str) -> Path:
    safe_repo = repo_id.replace("/", "__")
    safe_revision = re.sub(r"[^A-Za-z0-9._-]+", "_", revision)
    digest = hashlib.sha1(f"{repo_id}@{revision}".encode("utf-8")).hexdigest()[:12]
    filename = f"{safe_repo}--{safe_revision}--{digest}.json"
    return metadata_cache_dir() / filename


def tensor_to_json(tensor: TensorInfo) -> Dict[str, object]:
    return {
        "shape": list(tensor.shape),
        "dtype": tensor.dtype,
        "nbytes": tensor.nbytes,
    }


def tensor_from_json(data: object) -> Optional[TensorInfo]:
    if not isinstance(data, dict):
        return None
    shape_raw = data.get("shape")
    dtype_raw = data.get("dtype")
    nbytes_raw = data.get("nbytes")
    if not isinstance(shape_raw, list):
        return None
    if not isinstance(dtype_raw, str):
        return None
    if not isinstance(nbytes_raw, int):
        return None
    try:
        shape = tuple(int(dim) for dim in shape_raw)
    except (TypeError, ValueError):
        return None
    return TensorInfo(shape=shape, dtype=dtype_raw, nbytes=nbytes_raw)


def get_remote_revision_id(api: HfApi, repo_id: str, revision: str) -> str:
    info = api.model_info(repo_id=repo_id, revision=revision)
    sha = getattr(info, "sha", None)
    if sha:
        return f"sha:{sha}"

    last_modified = getattr(info, "last_modified", None)
    if last_modified is not None:
        if hasattr(last_modified, "isoformat"):
            return f"last_modified:{last_modified.isoformat()}"
        return f"last_modified:{last_modified}"

    return f"revision:{revision}"


def load_metadata_cache(repo_id: str, revision: str) -> Optional[MetadataCacheEntry]:
    path = metadata_cache_path(repo_id=repo_id, revision=revision)
    if not path.exists():
        return None

    try:
        with path.open("r", encoding="utf-8") as handle:
            payload = json.load(handle)
    except (OSError, json.JSONDecodeError):
        return None

    if not isinstance(payload, dict):
        return None
    if payload.get("schema_version") != CACHE_SCHEMA_VERSION:
        return None
    if payload.get("repo_id") != repo_id:
        return None
    if payload.get("revision") != revision:
        return None
    remote_revision_id = payload.get("remote_revision_id")
    if not isinstance(remote_revision_id, str) or not remote_revision_id:
        return None
    checked_at_raw = payload.get("revision_checked_at_epoch")
    if not isinstance(checked_at_raw, int):
        return None

    files_raw = payload.get("files")
    tensors_raw = payload.get("tensors")
    tensor_files_raw = payload.get("tensor_files")
    if not isinstance(files_raw, list) or not all(isinstance(value, str) for value in files_raw):
        return None
    if not isinstance(tensors_raw, dict):
        return None
    if not isinstance(tensor_files_raw, dict):
        return None

    tensors: Dict[str, TensorInfo] = {}
    for tensor_name, tensor_data in tensors_raw.items():
        if not isinstance(tensor_name, str):
            return None
        tensor = tensor_from_json(tensor_data)
        if tensor is None:
            return None
        tensors[tensor_name] = tensor

    tensor_files: Dict[str, str] = {}
    for tensor_name, filename in tensor_files_raw.items():
        if not isinstance(tensor_name, str) or not isinstance(filename, str):
            return None
        tensor_files[tensor_name] = filename

    return MetadataCacheEntry(
        files=list(files_raw),
        tensors=tensors,
        tensor_files=tensor_files,
        remote_revision_id=remote_revision_id,
        revision_checked_at_epoch=checked_at_raw,
    )


def save_metadata_cache(
    repo_id: str,
    revision: str,
    remote_revision_id: str,
    revision_checked_at_epoch: int,
    files: List[str],
    tensors: Dict[str, TensorInfo],
    tensor_files: Dict[str, str],
) -> None:
    path = metadata_cache_path(repo_id=repo_id, revision=revision)
    path.parent.mkdir(parents=True, exist_ok=True)

    payload = {
        "schema_version": CACHE_SCHEMA_VERSION,
        "repo_id": repo_id,
        "revision": revision,
        "remote_revision_id": remote_revision_id,
        "revision_checked_at_epoch": revision_checked_at_epoch,
        "files": files,
        "tensors": {name: tensor_to_json(tensor) for name, tensor in tensors.items()},
        "tensor_files": tensor_files,
    }

    tmp_path = path.with_name(f"{path.name}.{os.getpid()}.{time.time_ns()}.tmp")
    with tmp_path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, sort_keys=True, separators=(",", ":"))
    tmp_path.replace(path)


def parse_safetensors_header_bytes(raw: bytes, filename: str) -> Dict[str, object]:
    if len(raw) < 8:
        raise RuntimeError(f"{filename}: invalid safetensors header prefix.")
    header_len = struct.unpack("<Q", raw[:8])[0]
    header_raw = raw[8 : 8 + header_len]
    if len(header_raw) != header_len:
        raise RuntimeError(f"{filename}: truncated safetensors header.")
    try:
        return json.loads(header_raw)
    except json.JSONDecodeError as exc:
        raise RuntimeError(f"{filename}: invalid JSON in safetensors header.") from exc


def normalize_weight_map(raw_weight_map: object, context_name: str) -> Dict[str, str]:
    if not isinstance(raw_weight_map, dict) or not raw_weight_map:
        raise RuntimeError(f"{context_name} does not contain a usable weight_map.")
    weight_map: Dict[str, str] = {}
    for tensor_name, filename in raw_weight_map.items():
        if isinstance(tensor_name, str) and isinstance(filename, str):
            weight_map[tensor_name] = filename
    if not weight_map:
        raise RuntimeError(f"{context_name} does not contain a valid tensor->file map.")
    return weight_map


def resolve_safetensor_files(
    api: HfApi, repo_id: str, revision: str, hf_token: Optional[str]
) -> Tuple[List[str], Optional[str], Optional[Dict[str, str]]]:
    files = api.list_repo_files(repo_id=repo_id, repo_type="model", revision=revision)

    index_files = sorted(name for name in files if name.endswith(".safetensors.index.json"))
    if index_files:
        preferred_index = "model.safetensors.index.json" if "model.safetensors.index.json" in index_files else index_files[0]
        fs = HfFileSystem(token=hf_token)
        with fs.open(f"{repo_id}/{preferred_index}", "rb", revision=revision) as handle:
            index_data = json.load(handle)
        normalized_weight_map = normalize_weight_map(
            index_data.get("weight_map"),
            context_name=f"Index file {preferred_index}",
        )
        return sorted(set(normalized_weight_map.values())), preferred_index, normalized_weight_map

    safetensor_files = sorted(name for name in files if name.endswith(".safetensors"))
    if safetensor_files:
        return safetensor_files, None, None

    raise RuntimeError("No .safetensors files found in this repository.")


def resolve_local_safetensor_files(local_path: Path) -> Tuple[Path, List[str], Optional[Dict[str, str]]]:
    candidate = local_path.expanduser()
    if not candidate.exists():
        raise RuntimeError(f"Local path does not exist: {local_path}")

    def relative_posix(path: Path, base_dir: Path) -> str:
        return path.relative_to(base_dir).as_posix()

    if candidate.is_file():
        if candidate.name.endswith(".safetensors.index.json"):
            base_dir = candidate.parent.resolve()
            with candidate.open("r", encoding="utf-8") as handle:
                index_data = json.load(handle)
            weight_map = normalize_weight_map(index_data.get("weight_map"), context_name=f"Index file {candidate.name}")
            return base_dir, sorted(set(weight_map.values())), weight_map
        if candidate.name.endswith(".safetensors"):
            return candidate.parent.resolve(), [candidate.name], None
        raise RuntimeError("Local file must be a .safetensors or .safetensors.index.json file.")

    base_dir = candidate.resolve()
    index_candidates = sorted(base_dir.rglob("*.safetensors.index.json"))
    if index_candidates:
        preferred_name = "model.safetensors.index.json"
        preferred = next((path for path in index_candidates if path.name == preferred_name), index_candidates[0])
        with preferred.open("r", encoding="utf-8") as handle:
            index_data = json.load(handle)
        weight_map_raw = normalize_weight_map(index_data.get("weight_map"), context_name=f"Index file {preferred.name}")
        weight_map = {
            tensor_name: Path(filename).as_posix()
            for tensor_name, filename in weight_map_raw.items()
        }
        return base_dir, sorted(set(weight_map.values())), weight_map

    safetensor_paths = sorted(base_dir.rglob("*.safetensors"))
    if safetensor_paths:
        files = [relative_posix(path, base_dir) for path in safetensor_paths]
        return base_dir, files, None

    raise RuntimeError("No .safetensors files found in local path.")


def read_safetensors_header(fs: HfFileSystem, repo_id: str, filename: str, revision: str) -> Dict[str, object]:
    with fs.open(f"{repo_id}/{filename}", "rb", revision=revision) as handle:
        prefix = handle.read(8)
        if len(prefix) != 8:
            raise RuntimeError(f"{filename}: invalid safetensors header prefix.")
        header_len = struct.unpack("<Q", prefix)[0]
        rest = handle.read(header_len)
    return parse_safetensors_header_bytes(prefix + rest, filename=filename)


def read_safetensors_header_local(base_dir: Path, filename: str) -> Dict[str, object]:
    path = (base_dir / filename).resolve()
    if not path.exists():
        raise RuntimeError(f"{filename}: referenced in index but missing from local path.")
    with path.open("rb") as handle:
        prefix = handle.read(8)
        if len(prefix) != 8:
            raise RuntimeError(f"{filename}: invalid safetensors header prefix.")
        header_len = struct.unpack("<Q", prefix)[0]
        rest = handle.read(header_len)
    return parse_safetensors_header_bytes(prefix + rest, filename=filename)


def dtype_nbytes(dtype: str) -> Optional[int]:
    sizes = {
        "BOOL": 1,
        "U8": 1,
        "I8": 1,
        "F8_E4M3FN": 1,
        "F8_E5M2": 1,
        "I16": 2,
        "U16": 2,
        "F16": 2,
        "BF16": 2,
        "I32": 4,
        "U32": 4,
        "F32": 4,
        "I64": 8,
        "U64": 8,
        "F64": 8,
    }
    return sizes.get(dtype)


def estimate_nbytes(shape: Iterable[int], dtype: str) -> int:
    item_size = dtype_nbytes(dtype)
    if item_size is None:
        return 0
    count = 1
    for dim in shape:
        count *= int(dim)
    return count * item_size


def collect_tensors(
    repo_id: str, files: List[str], revision: str, hf_token: Optional[str]
) -> Tuple[Dict[str, TensorInfo], Dict[str, str]]:
    fs = HfFileSystem(token=hf_token)
    tensors: Dict[str, TensorInfo] = {}
    tensor_files: Dict[str, str] = {}

    for filename in files:
        header = read_safetensors_header(fs=fs, repo_id=repo_id, filename=filename, revision=revision)
        for tensor_name, tensor_meta in header.items():
            if tensor_name == "__metadata__":
                continue
            if not isinstance(tensor_meta, dict):
                continue

            shape_raw = tensor_meta.get("shape", [])
            shape = tuple(int(dim) for dim in shape_raw) if isinstance(shape_raw, list) else tuple()
            dtype = str(tensor_meta.get("dtype", "UNKNOWN"))

            offsets = tensor_meta.get("data_offsets")
            if isinstance(offsets, list) and len(offsets) == 2:
                nbytes = int(offsets[1]) - int(offsets[0])
            else:
                nbytes = estimate_nbytes(shape=shape, dtype=dtype)

            if tensor_name in tensors:
                continue

            tensors[tensor_name] = TensorInfo(shape=shape, dtype=dtype, nbytes=nbytes)
            tensor_files[tensor_name] = filename

    return tensors, tensor_files


def collect_tensors_local(base_dir: Path, files: List[str]) -> Tuple[Dict[str, TensorInfo], Dict[str, str]]:
    tensors: Dict[str, TensorInfo] = {}
    tensor_files: Dict[str, str] = {}

    for filename in files:
        header = read_safetensors_header_local(base_dir=base_dir, filename=filename)
        for tensor_name, tensor_meta in header.items():
            if tensor_name == "__metadata__":
                continue
            if not isinstance(tensor_meta, dict):
                continue

            shape_raw = tensor_meta.get("shape", [])
            shape = tuple(int(dim) for dim in shape_raw) if isinstance(shape_raw, list) else tuple()
            dtype = str(tensor_meta.get("dtype", "UNKNOWN"))

            offsets = tensor_meta.get("data_offsets")
            if isinstance(offsets, list) and len(offsets) == 2:
                nbytes = int(offsets[1]) - int(offsets[0])
            else:
                nbytes = estimate_nbytes(shape=shape, dtype=dtype)

            if tensor_name in tensors:
                continue

            tensors[tensor_name] = TensorInfo(shape=shape, dtype=dtype, nbytes=nbytes)
            tensor_files[tensor_name] = filename

    return tensors, tensor_files


def natural_sort_key(value: str) -> List[object]:
    parts = re.split(r"(\d+)", value)
    key: List[object] = []
    for part in parts:
        if part.isdigit():
            key.append(int(part))
        else:
            key.append(part)
    return key


def build_trie(tensors: Dict[str, TensorInfo]) -> TrieNode:
    root = TrieNode()
    for tensor_name, tensor in sorted(tensors.items(), key=lambda item: natural_sort_key(item[0])):
        parts = tensor_name.split(".")
        node = root
        for part in parts[:-1]:
            node = node.children.setdefault(part, TrieNode())
        leaf = node.children.setdefault(parts[-1], TrieNode())
        leaf.tensor = tensor
    return root


def clone_trie(node: TrieNode) -> TrieNode:
    cloned = TrieNode(tensor=node.tensor)
    for name, child in node.children.items():
        cloned.children[name] = clone_trie(child)
    return cloned


def filter_trie(root: TrieNode, query: Optional[str]) -> TrieNode:
    if not query:
        return root

    trimmed = query.strip()
    if not trimmed:
        return root

    # Dotted queries are treated as torch-style paths (exact segment matching),
    # e.g. model.layers.1 -> only that module/tensor subtree.
    if "." in trimmed:
        query_parts = [part.lower() for part in trimmed.split(".") if part]

        def walk_path(node: TrieNode, lower_path_parts: List[str]) -> Optional[TrieNode]:
            if lower_path_parts != query_parts[: len(lower_path_parts)]:
                return None
            if len(lower_path_parts) == len(query_parts):
                return clone_trie(node)

            filtered = TrieNode()
            for name, child in node.children.items():
                kept_child = walk_path(child, lower_path_parts + [name.lower()])
                if kept_child is not None:
                    filtered.children[name] = kept_child

            if not filtered.children:
                return None
            return filtered

        kept_root = walk_path(root, [])
    else:
        needle = trimmed.lower()

        def walk_substring(node: TrieNode, path_parts: List[str]) -> Optional[TrieNode]:
            path = ".".join(path_parts).lower()
            if path_parts and needle in path:
                return clone_trie(node)

            filtered = TrieNode()
            for name, child in node.children.items():
                kept_child = walk_substring(child, path_parts + [name])
                if kept_child is not None:
                    filtered.children[name] = kept_child

            if not filtered.children:
                return None
            return filtered

        kept_root = walk_substring(root, [])

    if kept_root is None:
        return TrieNode()
    return kept_root


def compute_signature(node: TrieNode, cache: Dict[int, object]) -> object:
    node_id = id(node)
    if node_id in cache:
        return cache[node_id]

    tensor_sig = None
    if node.tensor is not None:
        tensor_sig = (node.tensor.shape, node.tensor.dtype, node.tensor.nbytes)

    child_sig = tuple(
        (name, compute_signature(child, cache))
        for name, child in sorted(node.children.items(), key=lambda item: natural_sort_key(item[0]))
    )
    signature = (tensor_sig, child_sig)
    cache[node_id] = signature
    return signature


def index_groups(node: TrieNode, signature_cache: Dict[int, object]) -> Optional[List[IndexGroup]]:
    if len(node.children) < 2:
        return None
    if not all(name.isdigit() for name in node.children):
        return None

    ordered = sorted((int(name), child) for name, child in node.children.items())
    groups: List[IndexGroup] = []

    current_indices: List[int] = [ordered[0][0]]
    current_rep = ordered[0][1]
    current_sig = compute_signature(current_rep, signature_cache)

    for idx, child in ordered[1:]:
        sig = compute_signature(child, signature_cache)
        if sig == current_sig:
            current_indices.append(idx)
            continue

        groups.append(IndexGroup(indices=current_indices, representative=current_rep))
        current_indices = [idx]
        current_rep = child
        current_sig = sig

    groups.append(IndexGroup(indices=current_indices, representative=current_rep))

    if all(len(group.indices) == 1 for group in groups):
        return None
    return groups


def format_shape(shape: Tuple[int, ...]) -> str:
    if not shape:
        return "scalar"
    return "x".join(str(dim) for dim in shape)


def format_bytes(num_bytes: int) -> str:
    if num_bytes < 1000:
        return f"{num_bytes} B"
    value = float(num_bytes)
    for unit in ("KB", "MB", "GB", "TB", "PB"):
        value /= 1000.0
        if value < 1000.0 or unit == "PB":
            return f"{value:.1f} {unit}"
    return f"{num_bytes} B"


def format_percent(value: float) -> str:
    return f"{value:.1f} %"


def format_collapsed_indices(indices: List[int]) -> str:
    if len(indices) == 1:
        return str(indices[0])
    first = indices[0]
    last = indices[-1]
    is_contiguous = (last - first + 1) == len(indices)
    if is_contiguous:
        return f"({first}-{last})"
    joined = ",".join(str(value) for value in indices)
    return f"({joined})"


def collect_tensor_names(root: TrieNode) -> List[str]:
    names: List[str] = []

    def walk(node: TrieNode, prefix: str) -> None:
        for name, child in node.children.items():
            path = f"{prefix}.{name}" if prefix else name
            if child.tensor is not None:
                names.append(path)
            if child.children:
                walk(child, path)

    walk(root, "")
    return names


def format_scientific(value: float) -> str:
    if math.isnan(value):
        return "nan"
    if math.isinf(value):
        return "inf"
    mantissa, exponent = f"{value:.1e}".split("e")
    return f"{mantissa}e{int(exponent)}"


def format_compact_value(value: float) -> str:
    if math.isnan(value):
        return "nan"
    if math.isinf(value):
        return "inf"
    abs_value = abs(value)
    if abs_value != 0.0 and (abs_value >= 1000.0 or abs_value < 1e-3):
        return format_scientific(value)
    return f"{value:.3g}"


def format_two_dp(value: float) -> str:
    if math.isnan(value):
        return "nan"
    if math.isinf(value):
        return "inf"
    rounded = round(value, 2)
    if rounded == 0:
        rounded = 0.0
    return f"{rounded:.2f}"


def compute_compression_stats(original_fp32, reconstructed_fp32) -> TensorCompressionStats:
    import torch

    original = torch.nan_to_num(original_fp32.reshape(-1).to(torch.float64), nan=0.0, posinf=0.0, neginf=0.0)
    reconstructed = torch.nan_to_num(reconstructed_fp32.reshape(-1).to(torch.float64), nan=0.0, posinf=0.0, neginf=0.0)

    min_val = float(original.min().item()) if original.numel() else 0.0
    mean_val = float(original.mean().item()) if original.numel() else 0.0
    max_val = float(original.max().item()) if original.numel() else 0.0

    diff = (reconstructed - original).abs()
    atol = float(diff.max().item()) if diff.numel() else 0.0

    centered_original = original - original.mean()
    centered_reconstructed = reconstructed - reconstructed.mean()
    pcc_denom = torch.linalg.norm(centered_original) * torch.linalg.norm(centered_reconstructed)
    pcc_denom_value = float(pcc_denom.item())
    if pcc_denom_value == 0.0:
        pcc = 1.0 if atol == 0.0 else 0.0
    else:
        pcc = float((torch.dot(centered_original, centered_reconstructed) / pcc_denom).item())

    return TensorCompressionStats(min_val=min_val, mean_val=mean_val, max_val=max_val, pcc=pcc, atol=atol)


def import_ttnn_quiet():
    # Quiet noisy import-time logging from ttnn/loguru and TT runtime.
    os.environ.setdefault("LOGURU_LEVEL", "ERROR")
    os.environ.setdefault("TTNN_LOG_LEVEL", "ERROR")
    os.environ.setdefault("TT_METAL_LOGGER_LEVEL", "ERROR")
    try:
        import ttnn  # pylint: disable=import-outside-toplevel
    except ModuleNotFoundError as exc:
        raise RuntimeError("Compression requires `ttnn` in the active Python environment.") from exc

    return ttnn


def run_compressions(
    source: SourceState,
    tensor_names: List[str],
    tensors: Dict[str, TensorInfo],
    tensor_files: Dict[str, str],
    compress_formats: List[str],
) -> Dict[str, Dict[str, TensorCompressionStats]]:
    if not tensor_names or not compress_formats:
        return {}

    import torch  # pylint: disable=import-outside-toplevel
    from safetensors import safe_open  # pylint: disable=import-outside-toplevel
    from tqdm import tqdm  # pylint: disable=import-outside-toplevel

    disable_progress_bars()
    ttnn = import_ttnn_quiet()
    format_runtime: Dict[str, Tuple[object, bool]] = {}
    for format_name in compress_formats:
        format_spec = COMPRESS_FORMAT_SPECS.get(format_name)
        if format_spec is None:
            raise RuntimeError(f"Unsupported compression format: {format_name}")
        dtype_attr = str(format_spec["dtype_attr"])
        transpose_before_compress = bool(format_spec["transpose_before_compress"])
        if not hasattr(ttnn, dtype_attr):
            raise RuntimeError(f"Active ttnn does not support compression format '{format_name}'.")
        format_runtime[format_name] = (getattr(ttnn, dtype_attr), transpose_before_compress)

    names_by_file: Dict[str, List[str]] = {}
    total_bytes = 0
    for tensor_name in tensor_names:
        info = tensors.get(tensor_name)
        filename = tensor_files.get(tensor_name)
        if info is None:
            raise RuntimeError(f"Missing tensor metadata for {tensor_name}")
        if filename is None:
            raise RuntimeError(f"Missing shard mapping for {tensor_name}")
        names_by_file.setdefault(filename, []).append(tensor_name)
        total_bytes += info.nbytes

    if not names_by_file:
        return {}
    total_bytes *= len(compress_formats)

    stats_by_format: Dict[str, Dict[str, TensorCompressionStats]] = {name: {} for name in compress_formats}
    progress_desc = compress_formats[0] if len(compress_formats) == 1 else ",".join(compress_formats)
    with tqdm(
        total=total_bytes,
        unit="B",
        unit_scale=True,
        unit_divisor=1000,
        desc=progress_desc,
        dynamic_ncols=True,
        leave=True,
    ) as progress:
        for filename in sorted(names_by_file):
            if source.is_local:
                if source.local_base_dir is None:
                    raise RuntimeError("Internal error: missing local base path for local source.")
                local_path = str((source.local_base_dir / filename).resolve())
            else:
                if source.repo_id is None or source.revision is None:
                    raise RuntimeError("Internal error: missing repo/revision for remote source.")
                local_path = hf_hub_download(
                    repo_id=source.repo_id,
                    filename=filename,
                    repo_type="model",
                    revision=source.revision,
                    token=source.hf_token,
                )
            with safe_open(local_path, framework="pt", device="cpu") as handle:
                for tensor_name in sorted(names_by_file[filename], key=natural_sort_key):
                    tensor_info = tensors[tensor_name]
                    original = handle.get_tensor(tensor_name)
                    original_fp32 = original.to(dtype=torch.float32)
                    for format_name in compress_formats:
                        ttnn_dtype, transpose_before_compress = format_runtime[format_name]
                        compression_input = original_fp32
                        if transpose_before_compress and original_fp32.ndim >= 2:
                            compression_input = original_fp32.transpose(-2, -1).contiguous()
                        tt_tensor = ttnn.from_torch(
                            compression_input,
                            dtype=ttnn_dtype,
                            layout=ttnn.TILE_LAYOUT,
                        )
                        roundtrip_fp32 = ttnn.to_torch(tt_tensor).to(dtype=torch.float32)
                        if transpose_before_compress and original_fp32.ndim >= 2:
                            roundtrip_fp32 = roundtrip_fp32.transpose(-2, -1).contiguous()
                        stats_by_format[format_name][tensor_name] = compute_compression_stats(original_fp32, roundtrip_fp32)
                        progress.update(tensor_info.nbytes)
                        if compression_input is not original_fp32:
                            del compression_input
                        del tt_tensor
                        del roundtrip_fp32
                    del original_fp32
                    del original

    return stats_by_format


def render_tree(
    root: TrieNode,
    total_bytes: int,
    compression_formats: Optional[List[str]] = None,
    compression_stats: Optional[Dict[str, Dict[str, TensorCompressionStats]]] = None,
) -> List[str]:
    compression_formats = compression_formats or []
    compression_stats = compression_stats or {}

    rows: List[DisplayRow] = []
    signature_cache: Dict[int, object] = {}
    include_compression = bool(compression_formats)

    def join_path(prefix: str, child: str) -> str:
        return f"{prefix}.{child}" if prefix else child

    def add_tensor_row(label: str, tensor: TensorInfo, multiplier: int, tensor_paths: List[str]) -> None:
        effective_nbytes = tensor.nbytes * multiplier
        percent = (100.0 * effective_nbytes / total_bytes) if total_bytes else 0.0
        row = DisplayRow(
            label=label,
            shape=format_shape(tensor.shape),
            dtype=tensor.dtype,
            size=format_bytes(tensor.nbytes),
            percent=format_percent(percent),
        )
        if include_compression:
            summary_stats: Optional[List[TensorCompressionStats]] = None
            for format_name in compression_formats:
                stats_for_format = compression_stats.get(format_name, {})
                matched_stats = [stats_for_format[path] for path in tensor_paths if path in stats_for_format]
                if not matched_stats:
                    continue

                if summary_stats is None:
                    summary_stats = matched_stats

                worst_pcc = min(stat.pcc for stat in matched_stats)
                worst_atol = max(stat.atol for stat in matched_stats)
                row.compression_metrics[format_name] = (f"{worst_pcc:.4f}", format_scientific(worst_atol))

            if summary_stats:
                min_value = min(stat.min_val for stat in summary_stats)
                mean_value = sum(stat.mean_val for stat in summary_stats) / len(summary_stats)
                max_value = max(stat.max_val for stat in summary_stats)
                row.min_value = format_two_dp(min_value)
                row.mean_value = format_two_dp(mean_value)
                row.max_value = format_two_dp(max_value)
        rows.append(row)

    def walk(node: TrieNode, indent: str, multiplier: int, path_prefixes: List[str]) -> None:
        groups = index_groups(node, signature_cache)
        if groups is not None and len(groups) == 1:
            group = groups[0]
            label = format_collapsed_indices(group.indices)
            rows.append(DisplayRow(label=f"{indent}{label}"))
            group_prefixes = [join_path(prefix, str(index)) for prefix in path_prefixes for index in group.indices]
            walk(group.representative, indent + "  ", multiplier * len(group.indices), group_prefixes)
            return
        if groups is not None:
            for group in groups:
                label = format_collapsed_indices(group.indices)
                rows.append(DisplayRow(label=f"{indent}{label}"))
                group_prefixes = [join_path(prefix, str(index)) for prefix in path_prefixes for index in group.indices]
                walk(group.representative, indent + "  ", multiplier * len(group.indices), group_prefixes)
            return

        for name, child in sorted(node.children.items(), key=lambda item: natural_sort_key(item[0])):
            child_prefixes = [join_path(prefix, name) for prefix in path_prefixes]
            if child.tensor is not None and not child.children:
                add_tensor_row(
                    label=f"{indent}{name}",
                    tensor=child.tensor,
                    multiplier=multiplier,
                    tensor_paths=child_prefixes,
                )
                continue

            # Special case: F8 packed weights commonly store an adjacent scale tensor.
            # Render these as a single logical weight row and hide weight_scale_inv.
            f8_weight_child = child.children.get("weight")
            scale_inv_child = child.children.get("weight_scale_inv")
            if (
                child.tensor is None
                and len(child.children) == 2
                and f8_weight_child is not None
                and scale_inv_child is not None
                and f8_weight_child.tensor is not None
                and not f8_weight_child.children
                and scale_inv_child.tensor is not None
                and not scale_inv_child.children
                and f8_weight_child.tensor.dtype.startswith("F8_")
            ):
                add_tensor_row(
                    label=f"{indent}{name}",
                    tensor=f8_weight_child.tensor,
                    multiplier=multiplier,
                    tensor_paths=[join_path(prefix, "weight") for prefix in child_prefixes],
                )
                continue

            weight_child = child.children.get("weight")
            if (
                child.tensor is None
                and len(child.children) == 1
                and weight_child is not None
                and weight_child.tensor is not None
                and not weight_child.children
            ):
                add_tensor_row(
                    label=f"{indent}{name}",
                    tensor=weight_child.tensor,
                    multiplier=multiplier,
                    tensor_paths=[join_path(prefix, "weight") for prefix in child_prefixes],
                )
                continue

            child_groups = index_groups(child, signature_cache)
            if child_groups is not None and len(child_groups) == 1:
                group = child_groups[0]
                collapse_label = format_collapsed_indices(group.indices)
                rows.append(DisplayRow(label=f"{indent}{name} {collapse_label}"))
                group_prefixes = [join_path(prefix, str(index)) for prefix in child_prefixes for index in group.indices]
                walk(group.representative, indent + "  ", multiplier * len(group.indices), group_prefixes)
                continue
            if child_groups is not None:
                rows.append(DisplayRow(label=f"{indent}{name}"))
                for group in child_groups:
                    group_label = format_collapsed_indices(group.indices)
                    rows.append(DisplayRow(label=f"{indent}  {group_label}"))
                    group_prefixes = [join_path(prefix, str(index)) for prefix in child_prefixes for index in group.indices]
                    walk(group.representative, indent + "    ", multiplier * len(group.indices), group_prefixes)
                continue

            rows.append(DisplayRow(label=f"{indent}{name}"))
            walk(child, indent + "  ", multiplier, child_prefixes)

    walk(root, "", 1, [""])
    label_width = max((len(row.label) for row in rows), default=0)
    shape_width = max((len(row.shape) for row in rows if row.shape is not None), default=0)
    dtype_width = max((len(row.dtype) for row in rows if row.dtype is not None), default=0)
    size_width = max((len(row.size) for row in rows if row.size is not None), default=0)
    percent_width = max((len(row.percent) for row in rows if row.percent is not None), default=0)
    min_width = max((len((row.min_value or "-")) for row in rows if row.shape is not None), default=1)
    mean_width = max((len((row.mean_value or "-")) for row in rows if row.shape is not None), default=1)
    max_width = max((len((row.max_value or "-")) for row in rows if row.shape is not None), default=1)
    pcc_widths: Dict[str, int] = {}
    atol_widths: Dict[str, int] = {}
    if include_compression:
        for format_name in compression_formats:
            pcc_widths[format_name] = max(
                (len(row.compression_metrics.get(format_name, ("-", "-"))[0]) for row in rows if row.shape is not None),
                default=1,
            )
            atol_widths[format_name] = max(
                (len(row.compression_metrics.get(format_name, ("-", "-"))[1]) for row in rows if row.shape is not None),
                default=1,
            )

    lines: List[str] = []
    for row in rows:
        if row.shape is None:
            lines.append(paint_label(row.label, has_tensor=False))
        else:
            label_cell = paint_label(row.label.ljust(label_width + 2), has_tensor=True)
            shape_cell = paint_shape(row.shape) + (" " * (shape_width - len(row.shape) + 2))
            dtype_cell = paint_dtype(row.dtype) + (" " * (dtype_width - len(row.dtype) + 2))
            size_cell = (" " * (size_width - len(row.size))) + paint_size(row.size)
            percent_cell = (" " * (percent_width - len(row.percent))) + paint_percent(row.percent)
            lines.append(
                f"{label_cell}"
                f"{shape_cell}"
                f"{dtype_cell}"
                f"{size_cell}  "
                f"{percent_cell}"
            )
            if include_compression:
                min_cell = paint_scientific_e((row.min_value or "-").rjust(min_width))
                mean_cell = paint_scientific_e((row.mean_value or "-").rjust(mean_width))
                max_cell = paint_scientific_e((row.max_value or "-").rjust(max_width))
                lines[-1] += (
                    f"{paint_dark('  |  ')}"
                    f"{min_cell}{paint_dark(' ≤ ')}{mean_cell}{paint_dark(' ≤ ')}{max_cell}"
                )
                for format_name in compression_formats:
                    pcc_text, atol_text = row.compression_metrics.get(format_name, ("-", "-"))
                    pcc_cell = paint_metric_number(pcc_text.rjust(pcc_widths[format_name]), COLOR_PCC)
                    atol_cell = paint_scientific_metric(
                        atol_text.rjust(atol_widths[format_name]), COLOR_ATOL, COLOR_ATOL_DARK
                    )
                    lines[-1] += (
                        f"{paint_dark('  |  ')}"
                        f"{paint_dark(f'{format_name}  ')}"
                        f"{pcc_cell}{paint_dark(' pcc')}  "
                        f"{atol_cell}{paint_dark(' atol')}"
                    )
    return lines


def do_list(raw_repo: str, revision: str, filter_query: Optional[str], compress_formats: List[str]) -> int:
    local_candidate = Path(raw_repo).expanduser()
    if local_candidate.exists():
        local_base_dir, files, _weight_map = resolve_local_safetensor_files(local_candidate)
        tensors, tensor_files = collect_tensors_local(base_dir=local_base_dir, files=files)
        source = SourceState(
            source_label=str(local_candidate.expanduser().absolute()),
            is_local=True,
            local_base_dir=local_base_dir,
            repo_id=None,
            revision=None,
            hf_token=None,
        )
    else:
        repo_id = normalize_repo_id(raw_repo)
        hf_token = resolve_hf_token()
        api = HfApi(token=hf_token)
        now_epoch = int(time.time())
        cached = load_metadata_cache(repo_id=repo_id, revision=revision)

        # Skip remote revision checks for recently-validated cache entries.
        if cached is not None and (now_epoch - cached.revision_checked_at_epoch) < REVISION_CHECK_TTL_SECONDS:
            files, tensors, tensor_files = cached.files, cached.tensors, cached.tensor_files
        else:
            remote_revision_id = get_remote_revision_id(api=api, repo_id=repo_id, revision=revision)

            if cached is not None and cached.remote_revision_id == remote_revision_id:
                files, tensors, tensor_files = cached.files, cached.tensors, cached.tensor_files
                save_metadata_cache(
                    repo_id=repo_id,
                    revision=revision,
                    remote_revision_id=remote_revision_id,
                    revision_checked_at_epoch=now_epoch,
                    files=files,
                    tensors=tensors,
                    tensor_files=tensor_files,
                )
            else:
                files, _index_file, _weight_map = resolve_safetensor_files(
                    api=api, repo_id=repo_id, revision=revision, hf_token=hf_token
                )
                tensors, tensor_files = collect_tensors(
                    repo_id=repo_id, files=files, revision=revision, hf_token=hf_token
                )
                save_metadata_cache(
                    repo_id=repo_id,
                    revision=revision,
                    remote_revision_id=remote_revision_id,
                    revision_checked_at_epoch=now_epoch,
                    files=files,
                    tensors=tensors,
                    tensor_files=tensor_files,
                )

        source = SourceState(
            source_label=repo_id,
            is_local=False,
            local_base_dir=None,
            repo_id=repo_id,
            revision=revision,
            hf_token=hf_token,
        )

    trie = build_trie(tensors)
    trie = filter_trie(trie, filter_query)

    compression_stats: Optional[Dict[str, Dict[str, TensorCompressionStats]]] = None
    if compress_formats:
        tensor_names = collect_tensor_names(trie)
        compression_stats = run_compressions(
            source=source,
            tensor_names=tensor_names,
            tensors=tensors,
            tensor_files=tensor_files,
            compress_formats=compress_formats,
        )

    total_bytes = sum(tensor.nbytes for tensor in tensors.values())
    file_word = "file" if len(files) == 1 else "files"
    tensor_word = "tensor" if len(tensors) == 1 else "tensors"
    revision_label = paint_dark("(local)") if source.is_local else paint_dark(f"({revision})")
    print(
        f"{paint(source.source_label, COLOR_WHITE)} {revision_label}"
        f"{paint_dark(' - ')}"
        f"{len(files)} {paint_dark(file_word)}"
        f"{paint_dark(', ')}"
        f"{len(tensors)} {paint_dark(tensor_word)}"
        f"{paint_dark(', ')}"
        f"{paint_total_size(format_bytes(total_bytes))}"
    )

    for line in render_tree(
        trie,
        total_bytes=total_bytes,
        compression_formats=compress_formats,
        compression_stats=compression_stats,
    ):
        print(line)
    return 0


def main() -> int:
    args = parse_args()
    repo_arg = args.list_repo if args.list_repo else args.repo
    if not repo_arg:
        print("usage: we <HF_URL_OR_REPO> [FILTER_QUERY] [--revision REVISION] [-c FORMAT]", file=sys.stderr)
        return 2

    filter_query = " ".join(args.filter_query).strip()
    if not filter_query:
        filter_query = None

    try:
        compress_formats = resolve_compress_formats(args.compress)
        return do_list(
            raw_repo=repo_arg,
            revision=args.revision,
            filter_query=filter_query,
            compress_formats=compress_formats,
        )
    except (ValueError, RuntimeError, HfHubHTTPError) as exc:
        print(f"error: {exc}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    raise SystemExit(main())
